{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZKRuJ6wSlK4",
        "outputId": "aa475c85-9d13-4912-d248-502130d3004c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 13.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.19.6)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbeyfOHlUsTM",
        "outputId": "804fb02e-c3fc-4395-df73-390e56be1e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Box2D\n",
            "  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 13.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: Box2D\n",
            "Successfully installed Box2D-2.3.10\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install gym\n",
        "!pip install Box2D\n",
        "!pip install pygame"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://keras.io/ko/initializers/ (참조링크) \n",
        "\n",
        "1. Critic Model의 Initializer를 glorot_uniform, lecun_uniform로 바꾸어 테스트 진행\n",
        "2. Actor Model의 Initializer를 RandomNormal에서, stddev의 값을 10의 거듭 제곱 단위로 바꿔 3개 set 학습 진행 \n",
        "\n",
        "Actormodel 학습 진행하고, Critic model 학습 진행하고, 두 개 최적값을 합해서 학습 진행 \n",
        "\n",
        "※ 이 때, 아마 log 데이터가 2개 폴터 쌓일텐데, 반드시 설정한 변수값 "
      ],
      "metadata": {
        "id": "tBhN069mpJ9M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "vgOBiDL-uiNR",
        "outputId": "a65ed8ea-252c-40b3-9ddd-7dbd9afb4e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 1/10000, score: -237.78052755013007, average: -237.78 \n",
            "episode: 2/10000, score: -79.51147518057948, average: -158.65 \n",
            "episode: 3/10000, score: -432.32362896118093, average: -249.87 \n",
            "episode: 4/10000, score: -229.65117199554118, average: -244.82 \n",
            "episode: 5/10000, score: -199.70669139891572, average: -235.79 \n",
            "episode: 6/10000, score: -52.08370831546314, average: -205.18 \n",
            "episode: 7/10000, score: -235.8897183526538, average: -209.56 \n",
            "episode: 8/10000, score: -89.7164536009717, average: -194.58 \n",
            "episode: 9/10000, score: -102.45434862507443, average: -184.35 \n",
            "episode: 10/10000, score: -137.97074130111235, average: -179.71 \n",
            "episode: 11/10000, score: -488.19852974130464, average: -207.75 \n",
            "episode: 12/10000, score: -476.4481134548683, average: -230.14 \n",
            "episode: 13/10000, score: -128.34994784615157, average: -222.31 \n",
            "episode: 14/10000, score: -114.91977554378462, average: -214.64 \n",
            "episode: 15/10000, score: -166.226064001679, average: -211.42 \n",
            "episode: 16/10000, score: -432.495962922843, average: -225.23 \n",
            "episode: 17/10000, score: -94.22770837113413, average: -217.53 \n",
            "episode: 18/10000, score: -310.3466376535714, average: -222.68 \n",
            "episode: 19/10000, score: -144.00321774350977, average: -218.54 \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-02c945e90ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mmodel_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPOAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0mmodel_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train as PPO, train every epesode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-02c945e90ef7>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    234\u001b[0m                     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-02c945e90ef7>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, states, actions, rewards, predictions, dones, next_states)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# Actor와 Critic 모델을 학습시켜줌.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m       \u001b[0mcount_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m       mode=mode)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m   \u001b[0;31m# Find beforehand arrays that need sparse-to-dense conversion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mconfigure_callbacks\u001b[0;34m(callbacks, model, do_validation, batch_size, epochs, steps_per_epoch, samples, verbose, count_mode, mode)\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;31m# Set callback model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0mcallback_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_callback_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   \u001b[0mcallback_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   set_callback_parameters(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_history\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    306\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     if all(\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         base_layer_utils.has_weights(v) for v in tf.nest.flatten(value)):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x648 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import random\n",
        "import gym\n",
        "import pylab\n",
        "import numpy as np\n",
        "import copy\n",
        "from gym.wrappers import GrayScaleObservation\n",
        "from gym.spaces import Box\n",
        "from gym import Wrapper\n",
        "import time\n",
        "# Deep neural network 학습을 위한 tensorflow \n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "from tensorboardX import SummaryWriter\n",
        "tf.compat.v1.disable_eager_execution() \n",
        "\n",
        "\n",
        "# GPU를 사용 가능하도록 설정해줌. (있다면)\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(gpus) > 0:\n",
        "    print(f'GPUs {gpus}')\n",
        "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    except RuntimeError: pass\n",
        "\n",
        "\n",
        "\n",
        "class ActorModel:\n",
        "    def __init__(self, lr, optimizer, inputShape, actionSpace):\n",
        "        input = Input(inputShape)\n",
        "        self.actionSpace = actionSpace\n",
        "        layer = Dense(128, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.1))(input)\n",
        "        layer = Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.1))(layer)\n",
        "        output = Dense(actionSpace, activation=\"softmax\")(layer) # flatten 후, 최종적인 output을 action space의 shape에 맞게 출력하도록함.\n",
        "\n",
        "        self.Actor = Model(inputs = input, outputs = output) #OUTPUT 값으로써 각 행동들을 취할 확률을 리턴한다. \n",
        "        self.Actor.compile(loss=self.loss, optimizer=optimizer(lr=lr))\n",
        "\n",
        "    # Actor model의 ppo loss를 tensorflow 모델로 넘겨주는 함수. \n",
        "    def loss(self, y_true, y_pred):\n",
        "        advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.actionSpace], y_true[:, 1+self.actionSpace:]\n",
        "        LOSS_CLIPPING = 0.05\n",
        "        ENTROPY_LOSS = 0.001\n",
        "\n",
        "        oldProb = actions * prediction_picks\n",
        "        newProb = actions * y_pred\n",
        "\n",
        "        oldProb = K.clip(oldProb, 1e-10, 1.0)\n",
        "        newProb = K.clip(newProb, 1e-10, 1.0)\n",
        "        ratio = K.exp(K.log(newProb) - K.log(oldProb))\n",
        "        p1 = ratio * advantages\n",
        "        p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
        "\n",
        "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
        "        entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
        "        entropy = ENTROPY_LOSS * K.mean(entropy)\n",
        "        \n",
        "        total = actor_loss - entropy\n",
        "        return total\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.Actor.predict(state)\n",
        "\n",
        "class CriticModel:\n",
        "    def __init__(self,  lr, optimizer, inputShape, actionSpace):\n",
        "        input = Input(inputShape) # state,reward 값이 들어옴\n",
        "        oldValue = Input(shape=(1,)) # 과거 실행한 state 값을 의미\n",
        "\n",
        "        layer = Dense(128, activation=\"relu\", kernel_initializer='he_uniform')(input)\n",
        "        layer = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(layer)\n",
        "        value = Dense(1, activation=None)(layer)\n",
        "\n",
        "        self.Critic = Model(inputs=[input, oldValue], outputs = value) # output 값으로써 Q_VALUE가 리턴된다. \n",
        "        self.Critic.compile(loss=[self.critic_PPO2_loss(oldValue)], optimizer=optimizer(lr=lr))\n",
        "\n",
        "    def critic_PPO2_loss(self, values):\n",
        "        def loss(y_true, y_pred):\n",
        "            LOSS_CLIPPING = 0.05\n",
        "            clipped_v_loss = values + K.clip(y_pred - values, -LOSS_CLIPPING, LOSS_CLIPPING)\n",
        "            valueLoss = K.mean((y_true - y_pred) ** 2) \n",
        "            return valueLoss\n",
        "        return loss\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self):\n",
        "        self.env_name = 'LunarLander-v2'\n",
        "        self.env = gym.make(self.env_name)\n",
        "        self.state_size = self.env.observation_space.shape\n",
        "        self.action_size = self.env.action_space.n\n",
        "        self.lr = 0.001 # 설정한 learning rate \n",
        "        self.epochs = 10 # training epochs\n",
        "        self.shuffle=False\n",
        "        self.train_batch = 1000\n",
        "        self.EPISODES = 10000 # 총 학습시킬 episode 타깃 수 \n",
        "        self.current_epi = 0 # 현재 몇 개의 episode를 학습시켰는지를 저장해주는 변수 \n",
        "        self.max_avg = 0 # 현재 최고 몇 점이 났는지를 저장해두는 변수 \n",
        "        self.optimizer = Adam\n",
        "        self.count_replay = 0\n",
        "        self.writer = SummaryWriter(comment=\"_\"+self.env_name+\"_\"+self.optimizer.__name__+\"_\"+str(self.lr))\n",
        "        \n",
        "        # matplot을 쓰기 위한 메모리 초기화 \n",
        "        self.scores_, self.episodes_, self.average_ = [], [], [] \n",
        "\n",
        "        # Actor-Critic 모델을 생성함 \n",
        "        self.Actor = ActorModel(lr=self.lr, optimizer = self.optimizer, inputShape=self.state_size, actionSpace = self.action_size)\n",
        "        self.Critic = CriticModel( lr=self.lr, optimizer = self.optimizer, inputShape=self.state_size, actionSpace = self.action_size)\n",
        "        \n",
        "        self.ActorName = f\"{self.env_name}_PPO_Actor.h5\" # 모델 파일을 저장하기 위한 변수. \n",
        "        self.CriticName = f\"{self.env_name}_PPO_Critic.h5\"\n",
        "\n",
        "    # Actor모델을 통해 예측한 값을 리턴해주는 함수입니다. \n",
        "    def act(self, state):\n",
        "        prediction = self.Actor.predict(state)[0] # 예측값 배열을 가져옴 \n",
        "        action = np.random.choice(self.action_size, p=prediction) # action들이 뽑힐 확률을 지정해준후, 뽑기. \n",
        "        action_one_hot = np.zeros([self.action_size])\n",
        "        action_one_hot[action] = 1 # 뽑힌 액션에 대해 1표시 by one hot encoding\n",
        "        return action, action_one_hot, prediction\n",
        "\n",
        "    def get_gaes(self, rewards, dones, values, next_values, gamma = 0.99, lamda = 0.9, normalize=True):\n",
        "      # 에피소드가 지날수록 gamma 값이 곱해진 reward가 리턴됨. \n",
        "        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
        "        deltas = np.stack(deltas)\n",
        "        gaes = copy.deepcopy(deltas)\n",
        "        for t in reversed(range(len(deltas) - 1)):\n",
        "            gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
        "\n",
        "        target = gaes + values\n",
        "        if normalize:\n",
        "            gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
        "        return np.vstack(gaes), np.vstack(target)\n",
        "\n",
        "    # 지난 Action을 Critic model이 평가 후, 이를 보정해주는 알고리즘이 담긴 함수.\n",
        "    # 적절한 학습을 위해 np.vstack을 통해 input shape에 맞는 형태로 바꾸어  input으로 전달.  \n",
        "    def replay(self, states, actions, rewards, predictions, dones, next_states):\n",
        "        states = np.vstack(states)\n",
        "        next_states = np.vstack(next_states)\n",
        "        actions = np.vstack(actions)\n",
        "        predictions = np.vstack(predictions)\n",
        "\n",
        "        # Critic method를 통해 reward 값을 얻어옴 \n",
        "        values = self.Critic.predict(states)\n",
        "        next_values = self.Critic.predict(next_states)\n",
        "\n",
        "        #GAE 보정을 통한 Reward값을 저장. \n",
        "        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values)) \n",
        "\n",
        "        # loss 함수에 쓰이기 적합하도록 배열을 가로로 결합하여줌. \n",
        "        y_true = np.hstack([advantages, predictions, actions])\n",
        "        \n",
        "        # Actor와 Critic 모델을 학습시켜줌. \n",
        "        actor_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
        "        critic_loss = self.Critic.Critic.fit([states, values], target, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
        "\n",
        "        self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(actor_loss.history['loss']), self.count_replay)\n",
        "        self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(critic_loss.history['loss']), self.count_replay)\n",
        "        self.count_replay += 1\n",
        "        \n",
        "    def load(self):\n",
        "        self.Actor.Actor.load_weights(self.ActorName)\n",
        "        self.Critic.Critic.load_weights(self.CriticName)\n",
        "\n",
        "    def save(self):\n",
        "        self.Actor.Actor.save_weights(self.ActorName)\n",
        "        self.Critic.Critic.save_weights(self.CriticName)\n",
        "        \n",
        "    pylab.figure(figsize=(18, 9))\n",
        "    pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n",
        "    def PlotModel(self, score, current_epi): \n",
        "        self.scores_.append(score)\n",
        "        self.episodes_.append(current_epi)\n",
        "        self.average_.append(sum(self.scores_[-50:]) / len(self.scores_[-50:])) \n",
        "        if str(current_epi)[-2:] == \"00\":# much faster than episode % 100\n",
        "            pylab.plot(self.episodes_, self.scores_, 'b')\n",
        "            pylab.plot(self.episodes_, self.average_, 'r')\n",
        "            pylab.title(self.env_name+\" PPO training cycle\", fontsize=18)\n",
        "            pylab.ylabel('Score', fontsize=18)\n",
        "            pylab.xlabel('Steps', fontsize=18)\n",
        "            try:\n",
        "                pylab.grid(True)\n",
        "                pylab.savefig(self.env_name+\".png\")\n",
        "            except OSError:\n",
        "                pass\n",
        "        # saving best models\n",
        "        if self.average_[-1] >= self.max_avg:\n",
        "            self.max_avg = self.average_[-1]\n",
        "            self.save()\n",
        "            SAVING = \"SAVING\"\n",
        "            # decreaate learning rate every saved model\n",
        "            self.lr *= 0.95\n",
        "            K.set_value(self.Actor.Actor.optimizer.learning_rate, self.lr)\n",
        "            K.set_value(self.Critic.Critic.optimizer.learning_rate, self.lr)\n",
        "        else:\n",
        "            SAVING = \"\"\n",
        "\n",
        "        return self.average_[-1], SAVING\n",
        "    \n",
        "\n",
        "    def run(self): # 매 batch마다 학습을 시행해주는 함수. \n",
        "        state = self.env.reset()\n",
        "        state = np.reshape(state, [1, self.state_size[0]])\n",
        "        done, score, SAVING = False, 0, ''\n",
        "        while True:\n",
        "            # ppo는 과거 값들도 참고하여 학습을 실시하므로, 이를 저장할 배열 선언.\n",
        "            states, next_states, actions, rewards, predictions, dones = [], [], [], [], [], []\n",
        "            for t in range(self.train_batch): # 설정해둔 MiniBatch 사이즈 만큼의 범위 내에서 학습을 진행함. \n",
        "                action, action_one_hot, prediction = self.act(state)\n",
        "                \n",
        "                next_state, reward, done, _ = self.env.step(action) # Actor가 예측한 action을 취해주고, 이를 env의 agent에게 전달. \n",
        "                # Critic모델에서 평가를 위해 next_state, reward 값은 저장해둠. \n",
        "                states.append(state) \n",
        "                next_states.append(np.reshape(next_state, [1, self.state_size[0]]))\n",
        "                actions.append(action_one_hot) # ont hot encoding된 action값을 저장 \n",
        "                rewards.append(reward)\n",
        "                dones.append(done)\n",
        "                predictions.append(prediction) \n",
        "                # 시행이 되었으므로, 현재 상태를 저장함. 이후 Critic 모델에 param으로 쓰임. \n",
        "                state = np.reshape(next_state, [1, self.state_size[0]])\n",
        "                score += reward\n",
        "                if done:\n",
        "                    self.current_epi += 1\n",
        "                    average, SAVING = self.PlotModel(score, self.current_epi) # \n",
        "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.current_epi, self.EPISODES, score, average, SAVING))\n",
        "                    self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.current_epi)\n",
        "                    self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.current_epi)\n",
        "                    \n",
        "\n",
        "                    state, done, score, SAVING = self.env.reset(), False, 0, ''\n",
        "                    state = np.reshape(state, [1, self.state_size[0]])\n",
        "                    \n",
        "            self.replay(states, actions, rewards, predictions, dones, next_states)\n",
        "            if self.current_epi >= self.EPISODES:\n",
        "                break\n",
        "        self.env.close()  \n",
        "\n",
        "    def test(self, test_episodes = 100):\n",
        "        self.load()\n",
        "        for e in range(100):\n",
        "            state = self.env.reset()\n",
        "            state = np.reshape(state, [1, self.state_size[0]])\n",
        "            done = False\n",
        "            score = 0\n",
        "            while not done:\n",
        "                self.env.render()\n",
        "                action = np.argmax(self.Actor.predict(state)[0])\n",
        "                state, reward, done, _ = self.env.step(action)\n",
        "                state = np.reshape(state, [1, self.state_size[0]])\n",
        "                score += reward\n",
        "                if done:\n",
        "                    print(\"episode: {}/{}, score: {}\".format(e, test_episodes, score))\n",
        "                    break\n",
        "        self.env.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_train = PPOAgent()\n",
        "    model_train.run() # train as PPO, train every epesode\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T8-nPVnaWlC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF24wTH0SkHj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}